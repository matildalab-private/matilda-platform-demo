apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: yolov3-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2023-01-27T14:39:34.405624',
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "https://drive.google.com/file/d/1qn0mLFV7NBbmw6-ZTuF_tN_oJxRHx-XR/view?usp=sharing",
      "name": "train_dataset_url", "optional": true}, {"default": "https://drive.google.com/file/d/1gUCWhls3ZyVurdYFl1iikRUlDHmKi4ZQ/view?usp=sharing",
      "name": "val_dataset_url", "optional": true}, {"default": "https://drive.google.com/drive/folders/1btey4JhgBRkoJneGKkvBLTDAuOMfPNmV?usp=share_link",
      "name": "checkpoint_url", "optional": true}, {"default": "yolov3_train_30.tf",
      "name": "checkpoint_name", "optional": true}, {"default": "https://drive.google.com/file/d/13SNqfX3z8N1-qt4PwaS4RL0O2SydN09j/view?usp=sharing",
      "name": "test_img_url", "optional": true}, {"default": "416", "name": "model_size",
      "optional": true}, {"default": "6", "name": "num_classes", "optional": true},
      {"default": "1", "name": "num_epochs", "optional": true}, {"default": "[\u2018missing_hole\u2019,
      \u2018mouse_bite\u2019, \u2018open_circuit\u2019, \u2018short\u2019, \u2018spur\u2019,
      \u2018spurious_copper\u2019]", "name": "class_names", "optional": true}], "name":
      "YOLOv3 pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: yolov3-pipeline
  templates:
  - name: load-test-img
    container:
      args: [--img-url, '{{inputs.parameters.test_img_url}}', --input-img, /tmp/outputs/input_img/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'gdown' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'gdown' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def load_test_img(img_url, input_img):
            import gdown
            gdown.download(img_url, output=input_img, quiet=True, fuzzy=True)
            print(f'download complete!')

        import argparse
        _parser = argparse.ArgumentParser(prog='Load test img', description='')
        _parser.add_argument("--img-url", dest="img_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--input-img", dest="input_img", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = load_test_img(**_parsed_args)
      image: python:3.7
    inputs:
      parameters:
      - {name: test_img_url}
    outputs:
      artifacts:
      - {name: load-test-img-input_img, path: /tmp/outputs/input_img/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--img-url", {"inputValue": "img_url"}, "--input-img", {"outputPath":
          "input_img"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''gdown'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''gdown'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef load_test_img(img_url, input_img):\n    import gdown\n    gdown.download(img_url,
          output=input_img, quiet=True, fuzzy=True)\n    print(f''download complete!'')\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Load test img'', description='''')\n_parser.add_argument(\"--img-url\",
          dest=\"img_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-img\",
          dest=\"input_img\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = load_test_img(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"name": "img_url", "type": "String"}], "name": "Load test img", "outputs":
          [{"name": "input_img", "type": "jpg"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"img_url": "{{inputs.parameters.test_img_url}}"}'}
  - name: load-train-data
    container:
      args: [--train-dataset-url, '{{inputs.parameters.train_dataset_url}}', --val-dataset-url,
        '{{inputs.parameters.val_dataset_url}}', --train-dataset, /tmp/outputs/train_dataset/data,
        --val-dataset, /tmp/outputs/val_dataset/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'gdown' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'gdown' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef load_train_data(\n    train_dataset_url, \n    val_dataset_url, \n \
        \   train_dataset, \n    val_dataset\n):\n    import gdown\n    gdown.download(train_dataset_url,\
        \ output=train_dataset, quiet=True, fuzzy=True)\n    gdown.download(val_dataset_url,\
        \ output=val_dataset, quiet=True, fuzzy=True)\n    print(f'download complete!')\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Load train data',\
        \ description='')\n_parser.add_argument(\"--train-dataset-url\", dest=\"train_dataset_url\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --val-dataset-url\", dest=\"val_dataset_url\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--train-dataset\", dest=\"train_dataset\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--val-dataset\"\
        , dest=\"val_dataset\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = load_train_data(**_parsed_args)\n"
      image: python:3.7
    inputs:
      parameters:
      - {name: train_dataset_url}
      - {name: val_dataset_url}
    outputs:
      artifacts:
      - {name: load-train-data-train_dataset, path: /tmp/outputs/train_dataset/data}
      - {name: load-train-data-val_dataset, path: /tmp/outputs/val_dataset/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--train-dataset-url", {"inputValue": "train_dataset_url"}, "--val-dataset-url",
          {"inputValue": "val_dataset_url"}, "--train-dataset", {"outputPath": "train_dataset"},
          "--val-dataset", {"outputPath": "val_dataset"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''gdown'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''gdown'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef load_train_data(\n    train_dataset_url, \n    val_dataset_url,
          \n    train_dataset, \n    val_dataset\n):\n    import gdown\n    gdown.download(train_dataset_url,
          output=train_dataset, quiet=True, fuzzy=True)\n    gdown.download(val_dataset_url,
          output=val_dataset, quiet=True, fuzzy=True)\n    print(f''download complete!'')\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Load train data'', description='''')\n_parser.add_argument(\"--train-dataset-url\",
          dest=\"train_dataset_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--val-dataset-url\",
          dest=\"val_dataset_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-dataset\",
          dest=\"train_dataset\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--val-dataset\", dest=\"val_dataset\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = load_train_data(**_parsed_args)\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "train_dataset_url", "type":
          "String"}, {"name": "val_dataset_url", "type": "String"}], "name": "Load
          train data", "outputs": [{"name": "train_dataset", "type": "Dataset"}, {"name":
          "val_dataset", "type": "Dataset"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"train_dataset_url": "{{inputs.parameters.train_dataset_url}}",
          "val_dataset_url": "{{inputs.parameters.val_dataset_url}}"}'}
  - name: load-weights
    container:
      args: [--checkpoint-url, '{{inputs.parameters.checkpoint_url}}', --pretrained-weights,
        /tmp/outputs/pretrained_weights/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'gdown' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'gdown' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def load_weights(checkpoint_url, pretrained_weights):
            import gdown
            gdown.download_folder(checkpoint_url, output=pretrained_weights, quiet=True, use_cookies=False)
            print(f'download complete!')

        import argparse
        _parser = argparse.ArgumentParser(prog='Load weights', description='')
        _parser.add_argument("--checkpoint-url", dest="checkpoint_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--pretrained-weights", dest="pretrained_weights", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = load_weights(**_parsed_args)
      image: python:3.7
    inputs:
      parameters:
      - {name: checkpoint_url}
    outputs:
      artifacts:
      - {name: load-weights-pretrained_weights, path: /tmp/outputs/pretrained_weights/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--checkpoint-url", {"inputValue": "checkpoint_url"}, "--pretrained-weights",
          {"outputPath": "pretrained_weights"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''gdown'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''gdown'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef load_weights(checkpoint_url, pretrained_weights):\n    import
          gdown\n    gdown.download_folder(checkpoint_url, output=pretrained_weights,
          quiet=True, use_cookies=False)\n    print(f''download complete!'')\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Load weights'', description='''')\n_parser.add_argument(\"--checkpoint-url\",
          dest=\"checkpoint_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pretrained-weights\",
          dest=\"pretrained_weights\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = load_weights(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"name":
          "checkpoint_url", "type": "String"}], "name": "Load weights", "outputs":
          [{"name": "pretrained_weights", "type": "Weights"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"checkpoint_url": "{{inputs.parameters.checkpoint_url}}"}'}
  - name: serve
    container:
      args: [--temp-var, /tmp/inputs/temp_var/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def serve(temp_var):
            if temp_var:
                print('Model served successfully.')
            else:
                print('There was an error serving the model.')

        import argparse
        _parser = argparse.ArgumentParser(prog='Serve', description='')
        _parser.add_argument("--temp-var", dest="temp_var", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = serve(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: test-output_img, path: /tmp/inputs/temp_var/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--temp-var", {"inputPath": "temp_var"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def serve(temp_var):\n    if temp_var:\n        print(''Model
          served successfully.'')\n    else:\n        print(''There was an error serving
          the model.'')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Serve'',
          description='''')\n_parser.add_argument(\"--temp-var\", dest=\"temp_var\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = serve(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"name":
          "temp_var", "type": "jpg"}], "name": "Serve"}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: test
    container:
      args: [--model-size, '{{inputs.parameters.model_size}}', --num-classes, '{{inputs.parameters.num_classes}}',
        --class-names, '{{inputs.parameters.class_names}}', --trained-weights, /tmp/inputs/trained_weights/data,
        --input-img, /tmp/inputs/input_img/data, --output-img, /tmp/outputs/output_img/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'yolov3-minimal' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'yolov3-minimal' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def test(
            model_size,
            num_classes,
            class_names,
            trained_weights,
            input_img,
            output_img
        ):
            from yolov3_minimal import transform_images, draw_outputs, YoloV3
            import tensorflow as tf
            import cv2
            import os

            SIZE = model_size
            NUM_CLASSES = num_classes

            model = YoloV3(SIZE, classes=NUM_CLASSES)
            model.load_weights(trained_weights+'/trained_weights.tf').expect_partial()
            print('trained weights loaded')

            img_raw = tf.image.decode_image(open(input_img, 'rb').read(), channels=3)
            img = tf.expand_dims(img_raw, 0)
            img = transform_images(img, SIZE)

            boxes, scores, classes, nums = model(img)
            print('inference done')

            img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)
            img = draw_outputs(img, (boxes, scores, classes, nums), class_names)
            os.mkdir(output_img)
            cv2.imwrite(output_img+'/output.jpg', img)

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Test', description='')
        _parser.add_argument("--model-size", dest="model_size", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--num-classes", dest="num_classes", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--class-names", dest="class_names", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--trained-weights", dest="trained_weights", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--input-img", dest="input_img", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-img", dest="output_img", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = test(**_parsed_args)
      image: python:3.7
    inputs:
      parameters:
      - {name: class_names}
      - {name: model_size}
      - {name: num_classes}
      artifacts:
      - {name: load-test-img-input_img, path: /tmp/inputs/input_img/data}
      - {name: train-model-trained_weights, path: /tmp/inputs/trained_weights/data}
    outputs:
      artifacts:
      - {name: test-output_img, path: /tmp/outputs/output_img/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--model-size", {"inputValue": "model_size"}, "--num-classes",
          {"inputValue": "num_classes"}, "--class-names", {"inputValue": "class_names"},
          "--trained-weights", {"inputPath": "trained_weights"}, "--input-img", {"inputPath":
          "input_img"}, "--output-img", {"outputPath": "output_img"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''yolov3-minimal'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''yolov3-minimal''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef test(\n    model_size,\n    num_classes,\n    class_names,\n    trained_weights,\n    input_img,\n    output_img\n):\n    from
          yolov3_minimal import transform_images, draw_outputs, YoloV3\n    import
          tensorflow as tf\n    import cv2\n    import os\n\n    SIZE = model_size\n    NUM_CLASSES
          = num_classes\n\n    model = YoloV3(SIZE, classes=NUM_CLASSES)\n    model.load_weights(trained_weights+''/trained_weights.tf'').expect_partial()\n    print(''trained
          weights loaded'')\n\n    img_raw = tf.image.decode_image(open(input_img,
          ''rb'').read(), channels=3)\n    img = tf.expand_dims(img_raw, 0)\n    img
          = transform_images(img, SIZE)\n\n    boxes, scores, classes, nums = model(img)\n    print(''inference
          done'')\n\n    img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n    img
          = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n    os.mkdir(output_img)\n    cv2.imwrite(output_img+''/output.jpg'',
          img)\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Test'',
          description='''')\n_parser.add_argument(\"--model-size\", dest=\"model_size\",
          type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-classes\",
          dest=\"num_classes\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--class-names\",
          dest=\"class_names\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--trained-weights\",
          dest=\"trained_weights\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--input-img\",
          dest=\"input_img\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-img\",
          dest=\"output_img\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = test(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"name":
          "model_size", "type": "Integer"}, {"name": "num_classes", "type": "Integer"},
          {"name": "class_names", "type": "typing.List[str]"}, {"name": "trained_weights",
          "type": "Weights"}, {"name": "input_img", "type": "jpg"}], "name": "Test",
          "outputs": [{"name": "output_img", "type": "jpg"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"class_names": "{{inputs.parameters.class_names}}",
          "model_size": "{{inputs.parameters.model_size}}", "num_classes": "{{inputs.parameters.num_classes}}"}'}
  - name: train-model
    container:
      args: [--model-size, '{{inputs.parameters.model_size}}', --num-classes, '{{inputs.parameters.num_classes}}',
        --num-epohcs, '{{inputs.parameters.num_epochs}}', --class-names, '{{inputs.parameters.class_names}}',
        --checkpoint-name, '{{inputs.parameters.checkpoint_name}}', --pretrained-weights,
        /tmp/inputs/pretrained_weights/data, --train-dataset, /tmp/inputs/train_dataset/data,
        --val-dataset, /tmp/inputs/val_dataset/data, --trained-weights, /tmp/outputs/trained_weights/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'yolov3-minimal' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'yolov3-minimal' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef train_model(\n    model_size,\n    num_classes,\n    num_epohcs,\n \
        \   class_names,\n    checkpoint_name,\n    pretrained_weights,\n    train_dataset,\n\
        \    val_dataset,\n    trained_weights\n):\n    from yolov3_minimal import\
        \ load_tfrecord_dataset, transform_images, transform_targets, YoloV3, YoloLoss,\
        \ freeze_all\n    import numpy as np\n    import tensorflow as tf\n    from\
        \ tensorflow.keras.callbacks import (\n        ReduceLROnPlateau,\n      \
        \  EarlyStopping,\n        ModelCheckpoint\n    )\n\n    SIZE = model_size\n\
        \    NUM_CLASSES = num_classes\n    NUM_EPOCHS = num_epohcs\n    LEARNING_RATE\
        \ = 1e-3\n\n    anchors = np.array([(10, 13), (16, 30), (33, 23), (30, 61),\
        \ (62, 45),(59, 119), (116, 90), (156, 198), (373, 326)],np.float32) / 416\n\
        \    anchor_masks = np.array([[6, 7, 8], [3, 4, 5], [0, 1, 2]])\n\n    train_dataset\
        \ = load_tfrecord_dataset(train_dataset, class_names, SIZE)\n    train_dataset\
        \ = train_dataset.shuffle(buffer_size=512)\n    train_dataset = train_dataset.batch(8)\n\
        \    train_dataset = train_dataset.map(lambda x, y: (\n        transform_images(x,\
        \ SIZE),\n        transform_targets(y, anchors, anchor_masks, SIZE)))\n  \
        \  train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\
        \n    val_dataset = load_tfrecord_dataset(val_dataset, class_names, SIZE)\n\
        \    val_dataset = val_dataset.batch(8)\n    val_dataset = val_dataset.map(lambda\
        \ x, y: (\n        transform_images(x, SIZE),\n        transform_targets(y,\
        \ anchors, anchor_masks, SIZE)))\n    val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\
        \n    model = YoloV3(SIZE, classes=NUM_CLASSES, training=True)\n    model.load_weights(pretrained_weights+'/'+checkpoint_name).expect_partial()\n\
        \    freeze_all(model.get_layer('yolo_darknet'))\n\n    optimizer = tf.keras.optimizers.legacy.Adam(lr=LEARNING_RATE)\n\
        \    loss = [YoloLoss(anchors[mask], classes=NUM_CLASSES) for mask in anchor_masks]\n\
        \    model.compile(optimizer=optimizer, loss=loss)\n\n    callbacks = [\n\
        \        ReduceLROnPlateau(verbose=1),\n        EarlyStopping(patience=5,\
        \ verbose=1),\n        ModelCheckpoint(\n            filepath=trained_weights+'/trained_weights.tf',\n\
        \            save_weights_only=True,\n            monitor='val_loss',\n  \
        \          mode='min',\n            save_best_only=True\n        )\n    ]\n\
        \n    import os\n    os.mkdir(trained_weights)\n\n    model.fit(train_dataset,\
        \ epochs=NUM_EPOCHS, callbacks=callbacks, validation_data=val_dataset)   \
        \ \n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train\
        \ model', description='')\n_parser.add_argument(\"--model-size\", dest=\"\
        model_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --num-classes\", dest=\"num_classes\", type=int, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--num-epohcs\", dest=\"num_epohcs\", type=int, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--class-names\", dest=\"\
        class_names\", type=json.loads, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--checkpoint-name\", dest=\"checkpoint_name\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pretrained-weights\"\
        , dest=\"pretrained_weights\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--train-dataset\", dest=\"train_dataset\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--val-dataset\"\
        , dest=\"val_dataset\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--trained-weights\", dest=\"trained_weights\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = train_model(**_parsed_args)\n"
      image: python:3.7
    inputs:
      parameters:
      - {name: checkpoint_name}
      - {name: class_names}
      - {name: model_size}
      - {name: num_classes}
      - {name: num_epochs}
      artifacts:
      - {name: load-weights-pretrained_weights, path: /tmp/inputs/pretrained_weights/data}
      - {name: load-train-data-train_dataset, path: /tmp/inputs/train_dataset/data}
      - {name: load-train-data-val_dataset, path: /tmp/inputs/val_dataset/data}
    outputs:
      artifacts:
      - {name: train-model-trained_weights, path: /tmp/outputs/trained_weights/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--model-size", {"inputValue": "model_size"}, "--num-classes",
          {"inputValue": "num_classes"}, "--num-epohcs", {"inputValue": "num_epohcs"},
          "--class-names", {"inputValue": "class_names"}, "--checkpoint-name", {"inputValue":
          "checkpoint_name"}, "--pretrained-weights", {"inputPath": "pretrained_weights"},
          "--train-dataset", {"inputPath": "train_dataset"}, "--val-dataset", {"inputPath":
          "val_dataset"}, "--trained-weights", {"outputPath": "trained_weights"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''yolov3-minimal'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''yolov3-minimal''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_model(\n    model_size,\n    num_classes,\n    num_epohcs,\n    class_names,\n    checkpoint_name,\n    pretrained_weights,\n    train_dataset,\n    val_dataset,\n    trained_weights\n):\n    from
          yolov3_minimal import load_tfrecord_dataset, transform_images, transform_targets,
          YoloV3, YoloLoss, freeze_all\n    import numpy as np\n    import tensorflow
          as tf\n    from tensorflow.keras.callbacks import (\n        ReduceLROnPlateau,\n        EarlyStopping,\n        ModelCheckpoint\n    )\n\n    SIZE
          = model_size\n    NUM_CLASSES = num_classes\n    NUM_EPOCHS = num_epohcs\n    LEARNING_RATE
          = 1e-3\n\n    anchors = np.array([(10, 13), (16, 30), (33, 23), (30, 61),
          (62, 45),(59, 119), (116, 90), (156, 198), (373, 326)],np.float32) / 416\n    anchor_masks
          = np.array([[6, 7, 8], [3, 4, 5], [0, 1, 2]])\n\n    train_dataset = load_tfrecord_dataset(train_dataset,
          class_names, SIZE)\n    train_dataset = train_dataset.shuffle(buffer_size=512)\n    train_dataset
          = train_dataset.batch(8)\n    train_dataset = train_dataset.map(lambda x,
          y: (\n        transform_images(x, SIZE),\n        transform_targets(y, anchors,
          anchor_masks, SIZE)))\n    train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n    val_dataset
          = load_tfrecord_dataset(val_dataset, class_names, SIZE)\n    val_dataset
          = val_dataset.batch(8)\n    val_dataset = val_dataset.map(lambda x, y: (\n        transform_images(x,
          SIZE),\n        transform_targets(y, anchors, anchor_masks, SIZE)))\n    val_dataset
          = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n    model
          = YoloV3(SIZE, classes=NUM_CLASSES, training=True)\n    model.load_weights(pretrained_weights+''/''+checkpoint_name).expect_partial()\n    freeze_all(model.get_layer(''yolo_darknet''))\n\n    optimizer
          = tf.keras.optimizers.legacy.Adam(lr=LEARNING_RATE)\n    loss = [YoloLoss(anchors[mask],
          classes=NUM_CLASSES) for mask in anchor_masks]\n    model.compile(optimizer=optimizer,
          loss=loss)\n\n    callbacks = [\n        ReduceLROnPlateau(verbose=1),\n        EarlyStopping(patience=5,
          verbose=1),\n        ModelCheckpoint(\n            filepath=trained_weights+''/trained_weights.tf'',\n            save_weights_only=True,\n            monitor=''val_loss'',\n            mode=''min'',\n            save_best_only=True\n        )\n    ]\n\n    import
          os\n    os.mkdir(trained_weights)\n\n    model.fit(train_dataset, epochs=NUM_EPOCHS,
          callbacks=callbacks, validation_data=val_dataset)    \n\nimport json\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Train model'', description='''')\n_parser.add_argument(\"--model-size\",
          dest=\"model_size\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-classes\",
          dest=\"num_classes\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-epohcs\",
          dest=\"num_epohcs\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--class-names\",
          dest=\"class_names\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--checkpoint-name\",
          dest=\"checkpoint_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pretrained-weights\",
          dest=\"pretrained_weights\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-dataset\",
          dest=\"train_dataset\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--val-dataset\",
          dest=\"val_dataset\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--trained-weights\",
          dest=\"trained_weights\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"name":
          "model_size", "type": "Integer"}, {"name": "num_classes", "type": "Integer"},
          {"name": "num_epohcs", "type": "Integer"}, {"name": "class_names", "type":
          "typing.List[str]"}, {"name": "checkpoint_name", "type": "String"}, {"name":
          "pretrained_weights", "type": "Weights"}, {"name": "train_dataset", "type":
          "Dataset"}, {"name": "val_dataset", "type": "Dataset"}], "name": "Train
          model", "outputs": [{"name": "trained_weights", "type": "Weights"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"checkpoint_name": "{{inputs.parameters.checkpoint_name}}",
          "class_names": "{{inputs.parameters.class_names}}", "model_size": "{{inputs.parameters.model_size}}",
          "num_classes": "{{inputs.parameters.num_classes}}", "num_epohcs": "{{inputs.parameters.num_epochs}}"}'}
  - name: yolov3-pipeline
    inputs:
      parameters:
      - {name: checkpoint_name}
      - {name: checkpoint_url}
      - {name: class_names}
      - {name: model_size}
      - {name: num_classes}
      - {name: num_epochs}
      - {name: test_img_url}
      - {name: train_dataset_url}
      - {name: val_dataset_url}
    dag:
      tasks:
      - name: load-test-img
        template: load-test-img
        arguments:
          parameters:
          - {name: test_img_url, value: '{{inputs.parameters.test_img_url}}'}
      - name: load-train-data
        template: load-train-data
        arguments:
          parameters:
          - {name: train_dataset_url, value: '{{inputs.parameters.train_dataset_url}}'}
          - {name: val_dataset_url, value: '{{inputs.parameters.val_dataset_url}}'}
      - name: load-weights
        template: load-weights
        arguments:
          parameters:
          - {name: checkpoint_url, value: '{{inputs.parameters.checkpoint_url}}'}
      - name: serve
        template: serve
        dependencies: [test]
        arguments:
          artifacts:
          - {name: test-output_img, from: '{{tasks.test.outputs.artifacts.test-output_img}}'}
      - name: test
        template: test
        dependencies: [load-test-img, train-model]
        arguments:
          parameters:
          - {name: class_names, value: '{{inputs.parameters.class_names}}'}
          - {name: model_size, value: '{{inputs.parameters.model_size}}'}
          - {name: num_classes, value: '{{inputs.parameters.num_classes}}'}
          artifacts:
          - {name: load-test-img-input_img, from: '{{tasks.load-test-img.outputs.artifacts.load-test-img-input_img}}'}
          - {name: train-model-trained_weights, from: '{{tasks.train-model.outputs.artifacts.train-model-trained_weights}}'}
      - name: train-model
        template: train-model
        dependencies: [load-train-data, load-weights]
        arguments:
          parameters:
          - {name: checkpoint_name, value: '{{inputs.parameters.checkpoint_name}}'}
          - {name: class_names, value: '{{inputs.parameters.class_names}}'}
          - {name: model_size, value: '{{inputs.parameters.model_size}}'}
          - {name: num_classes, value: '{{inputs.parameters.num_classes}}'}
          - {name: num_epochs, value: '{{inputs.parameters.num_epochs}}'}
          artifacts:
          - {name: load-train-data-train_dataset, from: '{{tasks.load-train-data.outputs.artifacts.load-train-data-train_dataset}}'}
          - {name: load-train-data-val_dataset, from: '{{tasks.load-train-data.outputs.artifacts.load-train-data-val_dataset}}'}
          - {name: load-weights-pretrained_weights, from: '{{tasks.load-weights.outputs.artifacts.load-weights-pretrained_weights}}'}
  arguments:
    parameters:
    - {name: train_dataset_url, value: 'https://drive.google.com/file/d/1qn0mLFV7NBbmw6-ZTuF_tN_oJxRHx-XR/view?usp=sharing'}
    - {name: val_dataset_url, value: 'https://drive.google.com/file/d/1gUCWhls3ZyVurdYFl1iikRUlDHmKi4ZQ/view?usp=sharing'}
    - {name: checkpoint_url, value: 'https://drive.google.com/drive/folders/1btey4JhgBRkoJneGKkvBLTDAuOMfPNmV?usp=share_link'}
    - {name: checkpoint_name, value: yolov3_train_30.tf}
    - {name: test_img_url, value: 'https://drive.google.com/file/d/13SNqfX3z8N1-qt4PwaS4RL0O2SydN09j/view?usp=sharing'}
    - {name: model_size, value: '416'}
    - {name: num_classes, value: '6'}
    - {name: num_epochs, value: '1'}
    - {name: class_names, value: "[\u2018missing_hole\u2019, \u2018mouse_bite\u2019\
        , \u2018open_circuit\u2019, \u2018short\u2019, \u2018spur\u2019, \u2018spurious_copper\u2019\
        ]"}
  serviceAccountName: pipeline-runner
